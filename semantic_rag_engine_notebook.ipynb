{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a0be38",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§  Semantic Search & RAG Engine for Enterprise Docs\n",
    "\n",
    "This notebook replicates the Streamlit app for performing semantic search and retrieval-augmented generation (RAG) on uploaded PDF documents. It supports both local Hugging Face models and OpenAI GPT.\n",
    "\n",
    "Developed by **Dr. Al Rey Villagracia**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install PyMuPDF sentence-transformers scikit-learn torch transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65909106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\\n\".join([page.get_text() for page in doc])\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def embed_chunks(chunks, model):\n",
    "    return model.encode(chunks)\n",
    "\n",
    "def search(query, model, index, chunks, top_k=5):\n",
    "    query_vec = model.encode([query])\n",
    "    sims = cosine_similarity(query_vec, index)[0]\n",
    "    top_indices = np.argsort(sims)[::-1][:top_k]\n",
    "    return [(chunks[i], sims[i]) for i in top_indices]\n",
    "\n",
    "def heuristic_recall(query, top_chunks):\n",
    "    query_lower = query.lower()\n",
    "    hits = sum(1 for chunk, _ in top_chunks if query_lower in chunk.lower())\n",
    "    return hits / len(top_chunks)\n",
    "\n",
    "def measure_latency(func, *args, **kwargs):\n",
    "    start = time.perf_counter()\n",
    "    result = func(*args, **kwargs)\n",
    "    end = time.perf_counter()\n",
    "    return result, end - start\n",
    "\n",
    "def generate_answer_local(query, top_chunks, tokenizer, model, max_tokens=256):\n",
    "    context = \"\\n\\n\".join([chunk for chunk, _ in top_chunks])\n",
    "    prompt = f\"\"\"Use the following context to answer the question:\n",
    "\n",
    "    ---CONTEXT START---\n",
    "    {context}\n",
    "    ---CONTEXT END---\n",
    "\n",
    "    Question: {query}\n",
    "    Answer:\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=True, top_p=0.95, temperature=0.7)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n",
    "\n",
    "def log_to_csv(query, llm_answer, retrieval_latency, llm_latency, recall, feedback=None, model_used=None, log_file=\"rag_query_log.csv\"):\n",
    "    fieldnames = [\"timestamp\", \"query\", \"llm_answer\", \"retrieval_latency\", \"llm_latency\", \"recall\", \"feedback\", \"model_used\"]\n",
    "    row = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"query\": query,\n",
    "        \"llm_answer\": llm_answer,\n",
    "        \"retrieval_latency\": retrieval_latency,\n",
    "        \"llm_latency\": llm_latency,\n",
    "        \"recall\": recall,\n",
    "        \"feedback\": feedback,\n",
    "        \"model_used\": model_used\n",
    "    }\n",
    "    file_exists = os.path.exists(log_file)\n",
    "    with open(log_file, mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacdc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load PDF and model\n",
    "pdf_path = \"sample_test_doc.pdf\"  # Replace with your own\n",
    "query = \"What are the KPIs mentioned?\"\n",
    "\n",
    "# Load embedding model\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load local LLM\n",
    "hf_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "llm = AutoModelForCausalLM.from_pretrained(hf_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Process document\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "chunks = chunk_text(text)\n",
    "embeddings = embed_chunks(chunks, embed_model)\n",
    "index = np.array(embeddings)\n",
    "\n",
    "# Semantic search\n",
    "top_chunks, retrieval_latency = measure_latency(search, query, embed_model, index, chunks)\n",
    "\n",
    "# Generate LLM answer\n",
    "answer, llm_latency = measure_latency(generate_answer_local, query, top_chunks, tokenizer, llm)\n",
    "\n",
    "# Recall score\n",
    "recall = heuristic_recall(query, top_chunks)\n",
    "\n",
    "# Show results\n",
    "print(\"Query:\", query)\n",
    "print(\"Answer:\", answer)\n",
    "print(f\"Retrieval Latency: {retrieval_latency:.2f}s | LLM Latency: {llm_latency:.2f}s | Recall@k: {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log to CSV\n",
    "log_to_csv(query, answer, retrieval_latency, llm_latency, recall, feedback=\"positive\", model_used=hf_model_name)\n",
    "print(\"âœ… Logged to rag_query_log.csv\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}