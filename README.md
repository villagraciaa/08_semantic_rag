---
title: Semantic Repo
emoji: ğŸš€
colorFrom: red
colorTo: red
sdk: docker
app_port: 8501
tags:
  - streamlit
app_file: app.py
pinned: false
short_description: Streamlit template space
license: mit
---

# Semantic Search & Retrieval-Augmented Generation (RAG) Engine for Enterprise Docs
## ğŸ‘¨â€ğŸ”¬ Author
Developed by **Dr. Al Rey Villagracia**

## ğŸ“Œ Project Title
**Semantic Search & RAG Engine for Enterprise Documents**

## ğŸ¯ Objectives
- Enable users to intelligently search and query enterprise PDF documents using natural language.
- Retrieve semantically relevant passages from uploaded documents.
- Generate accurate, context-aware answers using either OpenAI GPT models or local Hugging Face language models.
- Log interactions and evaluate performance via recall, latency, and user feedback metrics.

## ğŸ“š Theory and Methodology

### ğŸ” Semantic Search
- Uses Sentence-BERT (e.g., `all-MiniLM-L6-v2`) to convert document chunks into dense vector representations.
- Employs cosine similarity to retrieve the top-k most relevant chunks based on a userâ€™s query.

### ğŸ§  Retrieval-Augmented Generation (RAG)
- Retrieved text chunks are fed into a language model (either OpenAI or Hugging Face local model).
- The model generates a natural language answer grounded in the retrieved content.
- This hybrid approach reduces hallucinations and improves factual grounding.

## ğŸ”§ Methodology Pipeline

```
[Upload PDF]
      â†“
[Extract Text using PyMuPDF]
      â†“
[Chunk and Embed using Sentence-BERT]
      â†“
[Store in Vector Index]
      â†“
[Semantic Search using Cosine Similarity]
      â†“
[Answer Generated by LLM with Retrieved Context]
      â†“
[Performance Metrics + Logging + Feedback]
```

## ğŸ§ª Key Features
- âœ… Support for **OpenAI GPT-3.5** (with secure API key input)
- âœ… Support for **local Hugging Face models** like Mistral, LLaMA, Falcon
- âœ… Chunk-level retrieval with highlight preview in full text
- âœ… Performance metrics: Retrieval latency, LLM latency, recall@k
- âœ… Logging of user queries, answers, and feedback in CSV format

## ğŸ“Š Insights from Test Case
- The engine successfully retrieved and highlighted relevant KPIs from a business report.
- Local models (e.g., Mistral 7B) were able to produce comparable answers to OpenAI GPT with slightly higher latency.
- User feedback logging enabled data-driven refinement of answer quality.

## ğŸš€ How to Run

### Install Dependencies
```
pip install -r requirements.txt
```

### Launch the Streamlit App
```
streamlit run semantic_rag_app_v2.py
```

### Run the CLI Test Case
```
python semantic_rag_app_testable.py
```

## ğŸ“‚ File Overview

| File                          | Description |
|-------------------------------|-------------|
| `semantic_rag_app_v2.py`      | Full Streamlit app |
| `semantic_rag_app_testable.py`| CLI test using a sample query and PDF |
| `sample_test_doc.pdf`         | Test document with KPIs and strategic notes |
| `rag_query_log.csv`           | CSV log for all RAG interactions |
| `requirements.txt`            | Python dependency list |
| `README.md`                   | This documentation file |


