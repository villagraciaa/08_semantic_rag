---
title: Semantic Repo
emoji: 🚀
colorFrom: red
colorTo: red
sdk: docker
app_port: 8501
tags:
  - streamlit
app_file: app.py
pinned: false
short_description: Streamlit template space
license: mit
---

# Semantic Search & Retrieval-Augmented Generation (RAG) Engine for Enterprise Docs
## 👨‍🔬 Author
Developed by **Dr. Al Rey Villagracia**

## 📌 Project Title
**Semantic Search & RAG Engine for Enterprise Documents**

## 🎯 Objectives
- Enable users to intelligently search and query enterprise PDF documents using natural language.
- Retrieve semantically relevant passages from uploaded documents.
- Generate accurate, context-aware answers using either OpenAI GPT models or local Hugging Face language models.
- Log interactions and evaluate performance via recall, latency, and user feedback metrics.

## 📚 Theory and Methodology

### 🔍 Semantic Search
- Uses Sentence-BERT (e.g., `all-MiniLM-L6-v2`) to convert document chunks into dense vector representations.
- Employs cosine similarity to retrieve the top-k most relevant chunks based on a user’s query.

### 🧠 Retrieval-Augmented Generation (RAG)
- Retrieved text chunks are fed into a language model (either OpenAI or Hugging Face local model).
- The model generates a natural language answer grounded in the retrieved content.
- This hybrid approach reduces hallucinations and improves factual grounding.

## 🔧 Methodology Pipeline

```
[Upload PDF]
      ↓
[Extract Text using PyMuPDF]
      ↓
[Chunk and Embed using Sentence-BERT]
      ↓
[Store in Vector Index]
      ↓
[Semantic Search using Cosine Similarity]
      ↓
[Answer Generated by LLM with Retrieved Context]
      ↓
[Performance Metrics + Logging + Feedback]
```

## 🧪 Key Features
- ✅ Support for **OpenAI GPT-3.5** (with secure API key input)
- ✅ Support for **local Hugging Face models** like Mistral, LLaMA, Falcon
- ✅ Chunk-level retrieval with highlight preview in full text
- ✅ Performance metrics: Retrieval latency, LLM latency, recall@k
- ✅ Logging of user queries, answers, and feedback in CSV format

## 📊 Insights from Test Case
- The engine successfully retrieved and highlighted relevant KPIs from a business report.
- Local models (e.g., Mistral 7B) were able to produce comparable answers to OpenAI GPT with slightly higher latency.
- User feedback logging enabled data-driven refinement of answer quality.

## 🚀 How to Run

### Install Dependencies
```
pip install -r requirements.txt
```

### Launch the Streamlit App
```
streamlit run semantic_rag_app_v2.py
```

### Run the CLI Test Case
```
python semantic_rag_app_testable.py
```

## 📂 File Overview

| File                          | Description |
|-------------------------------|-------------|
| `semantic_rag_app_v2.py`      | Full Streamlit app |
| `semantic_rag_app_testable.py`| CLI test using a sample query and PDF |
| `sample_test_doc.pdf`         | Test document with KPIs and strategic notes |
| `rag_query_log.csv`           | CSV log for all RAG interactions |
| `requirements.txt`            | Python dependency list |
| `README.md`                   | This documentation file |


